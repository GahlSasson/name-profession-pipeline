name: discovery

on:
  workflow_dispatch:
    inputs:
      clusters:
        description: "Space-separated clusters (e.g. 'Trades Arts')"
        required: true
        default: "Trades"
      limit:
        description: "Max results per occupation query"
        required: true
        default: "60"
      langs:
        description: "Languages for labels (comma-separated, e.g. en,de,fr)"
        required: true
        default: "en,de,fr"
      mode:
        description: "open (surname filter only) or strict (adds occupation filter)"
        required: true
        default: "open"

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install pandas SPARQLWrapper rapidfuzz Metaphone requests

      - name: Show chosen inputs
        run: |
          echo "clusters: ${{ inputs.clusters }}"
          echo "limit:    ${{ inputs.limit }}"
          echo "langs:    ${{ inputs.langs }}"
          echo "mode:     ${{ inputs.mode }}"

      - name: Show SPARQL template header (sanity)
        run: |
          echo "Template first 20 lines:"
          sed -n '1,20p' queries/query_template.sparql

      - name: Run discovery (OPEN/STRICT with safe filters)
        run: |
          set -e
          mkdir -p data
          echo "Running discovery…"
          python scripts/wikidata_nomen_agent.py \
            --mode "${{ inputs.mode }}" \
            --clusters ${{ inputs.clusters }} \
            --limit ${{ inputs.limit }} \
            --langs "${{ inputs.langs }}" \
            --outfile data/candidates_raw.csv

      - name: Debug output (if any)
        if: ${{ always() }}
        run: |
          echo "Data dir contents:"; ls -l data || true
          echo "CSV line count:";    wc -l data/candidates_raw.csv || true
          echo "First 5 lines:";     head -n 5 data/candidates_raw.csv || true

      # Optional: ensure the Upload step runs at least once so you see mapping
      - name: Fallback: seed 1 demo row if no CSV (optional)
        if: ${{ hashFiles('data/candidates_raw.csv') == '' }}
        run: |
          echo "Seeding a demo row so the upload step runs and shows mapping…"
          echo 'person,personLabel,surnameLabel,occupationLabel,cluster' > data/candidates_raw.csv
          echo 'https://www.wikidata.org/wiki/Q99999999,Test Baker,Baker,Baker,Trades' >> data/candidates_raw.csv

      - name: Save CSV as artifact
        if: ${{ hashFiles('data/candidates_raw.csv') != '' }}
        uses: actions/upload-artifact@v4
        with:
          name: discovery-csv
          path: data/candidates_raw.csv

      - name: Upload to Airtable (only if CSV exists)
        if: ${{ hashFiles('data/candidates_raw.csv') != '' }}
        env:
          AIRTABLE_API_KEY: ${{ secrets.AIRTABLE_API_KEY }}
          AIRTABLE_BASE_ID: ${{ secrets.AIRTABLE_BASE_ID }}
          AIRTABLE_TABLE_NAME: ${{ secrets.AIRTABLE_TABLE_NAME }}
        run: |
          echo "Using base: $AIRTABLE_BASE_ID ; table: $AIRTABLE_TABLE_NAME"
          python scripts/push_to_airtable.py --csv data/candidates_raw.csv

      - name: No CSV produced
        if: ${{ hashFiles('data/candidates_raw.csv') == '' }}
        run: |
          echo "No CSV produced by discovery (and fallback disabled); upload step was skipped."
