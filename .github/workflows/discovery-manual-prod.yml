      - name: Upload CSV (field-aware; respects single-select options)
        run: |
          python - <<'PY'
          import os, sys, csv, json, time, urllib.parse, requests, string

          BASE   = os.getenv("AIRTABLE_BASE_ID")
          TOKEN  = (os.getenv("AIRTABLE_TOKEN") or os.getenv("AIRTABLE_API_KEY") or "")
          TABLE  = os.getenv("AIRTABLE_TABLE_ID") or os.getenv("AIRTABLE_TABLE_NAME")
          if not (BASE and TOKEN and TABLE): sys.exit("Missing AIRTABLE_* envs.")

          # sanitize token
          TOKEN = TOKEN.strip().replace("\r","").replace("\n","").replace("\t","")
          H_AUTH = {"Authorization": f"Bearer {TOKEN}"}
          H_JSON = {"Authorization": f"Bearer {TOKEN}", "Content-Type": "application/json"}
          enc_table = urllib.parse.quote(TABLE, safe="")
          url = f"https://api.airtable.com/v0/{BASE}/{enc_table}"   # NOTE: no ?typecast=true

          # --- discover fields + options (type, choices) ---
          def discover_schema():
              r = requests.get(f"https://api.airtable.com/v0/meta/bases/{BASE}/tables", headers=H_AUTH, timeout=30)
              fields, defs = [], {}
              if r.status_code == 200:
                  for t in r.json().get("tables", []):
                      if t.get("id") == TABLE or t.get("name") == TABLE:
                          for f in t.get("fields", []):
                              fields.append(f["name"])
                              defs[f["name"]] = f
                          return fields, defs
              # fallback: just list fields seen in existing records
              r = requests.get(f"https://api.airtable.com/v0/{BASE}/{enc_table}?maxRecords=5", headers=H_AUTH, timeout=30)
              seen, defs = set(), {}
              if r.status_code == 200:
                  for rec in r.json().get("records", []):
                      seen.update((rec.get("fields") or {}).keys())
              return sorted(seen), defs

          avail, defs = discover_schema()
          print("[UPLOAD] Available fields:", avail or "(none)")
          # helper to pick fields by common aliases
          def pick_one(cands):
              for c in cands:
                  for f in avail:
                      if f.lower()==c.lower(): return f
              for c in cands:
                  cl=c.lower()
                  for f in avail:
                      if cl in f.lower(): return f
              return None
          def pick_all(cands):
              out, seen=[], set()
              for c in cands:
                  for f in avail:
                      if f.lower()==c.lower() and f not in seen: out.append(f); seen.add(f)
              for c in cands:
                  cl=c.lower()
                  for f in avail:
                      if cl in f.lower() and f not in seen: out.append(f); seen.add(f)
              return out

          full_name_field  = pick_one(["full_name","Full Name","Name","fullname","Full name","title"])
          cluster_field    = pick_one(["profession_cluster","professional_cluster","Profession Cluster","cluster","Cluster","category","Category"])
          occupation_field = pick_one(["occupation","profession_canonical","professional_canonical","professional_canonoical","job","role","Role"])
          language_fields  = pick_all(["language","Language","language-origin","Language Origin","lang","Lang"])

          # read allowed options for single-select cluster (if schema available)
          allowed_cluster = None
          if cluster_field and cluster_field in defs:
              fdef = defs[cluster_field]
              if fdef.get("type") in ("singleSelect","multipleSelects"):
                  choices = (fdef.get("options") or {}).get("choices") or []
                  allowed_cluster = [c.get("name") for c in choices if "name" in c]
                  print(f"[UPLOAD] {cluster_field} allowed options:", allowed_cluster)

          def normalize_select(value, allowed):
              if not value or not allowed: return None
              for opt in allowed:
                  if value.lower() == opt.lower():
                      return opt  # return canonical casing
              return None  # not allowed → skip

          # build records from CSV
          recs=[]
          with open("data/candidates_raw.csv", newline="", encoding="utf-8") as f:
              reader=csv.DictReader(f)
              for row in reader:
                  fields={}
                  if full_name_field and row.get("full_name"):  fields[full_name_field]=row["full_name"]
                  if occupation_field and row.get("occupation"): fields[occupation_field]=row["occupation"]
                  # cluster: only set if value is in allowed options
                  if cluster_field and row.get("cluster"):
                      if allowed_cluster is None:
                          # schema not available → be conservative and skip to avoid 422
                          pass
                      else:
                          norm = normalize_select(row["cluster"], allowed_cluster)
                          if norm is not None:
                              fields[cluster_field] = norm
                          else:
                              print(f"[UPLOAD] Skipping cluster '{row['cluster']}' (not in allowed options).")
                  if language_fields and row.get("lang"):
                      for lf in language_fields: fields[lf]=row["lang"]
                  # last resort: if fields empty, set a likely primary
                  if not fields:
                      nf = pick_one(["Name","Full Name","full_name","Title","title"])
                      if nf: fields[nf] = row.get("full_name") or "Record"
                  recs.append({"fields": fields})

          # upload in batches of 10
          total=0
          for i in range(0, len(recs), 10):
              batch=recs[i:i+10]
              r=requests.post(url, headers=H_JSON, data=json.dumps({"records": batch}), timeout=60)
              if r.status_code >= 400:
                  print(r.text); sys.exit(1)
              total += len(batch)
              print(f"[UPLOAD] batch {i//10+1}: {len(batch)}")
              time.sleep(0.25)
          print(f"[UPLOAD] inserted {total} rows")
          PY
