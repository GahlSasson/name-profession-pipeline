name: Discovery Manual PROD
on:
  workflow_dispatch:
    inputs:
      clusters: { description: "Comma-separated clusters", required: true, default: "Trades" }
      limit:    { description: "Per-occupation result limit", required: true, default: "5" }
      langs:    { description: "Comma-separated language codes", required: true, default: "en" }

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      AIRTABLE_TOKEN:      ${{ secrets.AIRTABLE_TOKEN }}       # you already set this
      AIRTABLE_API_KEY:    ${{ secrets.AIRTABLE_API_KEY }}     # optional fallback (not required)
      AIRTABLE_BASE_ID:    ${{ secrets.AIRTABLE_BASE_ID }}     # apprubc2DbwI5i9c1
      AIRTABLE_TABLE_ID:   ${{ secrets.AIRTABLE_TABLE_ID }}    # optional; we’ll use NAME if not set
      AIRTABLE_TABLE_NAME: ${{ secrets.AIRTABLE_TABLE_NAME }}  # you already set this
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - name: Install requests
        run: python -m pip install --upgrade pip && python -m pip install requests

      # ---- Generate a small CSV (replace this block later with your discovery step) ----
      - name: Generate CSV (mock data for pipeline test)
        env:
          CLUSTERS: ${{ github.event.inputs.clusters }}
          LIMIT:    ${{ github.event.inputs.limit }}
          LANGS:    ${{ github.event.inputs.langs }}
        run: |
          python - <<'PY'
          import os, csv, pathlib
          clusters=[c.strip() for c in os.getenv("CLUSTERS","").split(",") if c.strip()]
          try: limit=int(os.getenv("LIMIT","5"))
          except: limit=5
          langs=[l.strip() for l in os.getenv("LANGS","en").split(",") if l.strip()]

          rows=[
            {"full_name":"Ada Lovelace","occupation":"Mathematician","cluster":"Science","lang":"en"},
            {"full_name":"Nikola Tesla","occupation":"Engineer","cluster":"Trades","lang":"en"},
            {"full_name":"Leonardo da Vinci","occupation":"Artist","cluster":"Arts","lang":"it"},
          ]
          rows=[r for r in rows if (not clusters or r["cluster"] in clusters) and (not langs or r["lang"] in langs)]
          if not rows:
            rows=[{"full_name":"Test Person","occupation":"Tester","cluster":(clusters[0] if clusters else "Trades"),"lang":(langs[0] if langs else "en")}]

          p=pathlib.Path("data"); p.mkdir(parents=True, exist_ok=True)
          with open("data/candidates_raw.csv","w",newline="",encoding="utf-8") as f:
            w=csv.DictWriter(f,fieldnames=["full_name","occupation","cluster","lang"])
            w.writeheader(); w.writerows(rows)
          print("[CSV] wrote", len(rows), "rows -> data/candidates_raw.csv")
          PY

      - name: Show CSV
        run: |
          echo "linecount:"; wc -l data/candidates_raw.csv
          echo "preview:"; head -n 5 data/candidates_raw.csv

      # ---- Upload CSV to Airtable (auto-detects your actual column names) ----
      - name: Upload CSV (field-aware; writes lang to all language* fields)
        run: |
          python - <<'PY'
          import os, sys, csv, json, time, urllib.parse, requests, string

          BASE   = os.getenv("AIRTABLE_BASE_ID")
          TOKEN  = (os.getenv("AIRTABLE_TOKEN") or os.getenv("AIRTABLE_API_KEY") or "")
          TABLE  = os.getenv("AIRTABLE_TABLE_ID") or os.getenv("AIRTABLE_TABLE_NAME")
          if not (BASE and TOKEN and TABLE): sys.exit("Missing AIRTABLE_* envs.")

          # sanitize token
          TOKEN = TOKEN.strip().replace("\r","").replace("\n","").replace("\t","")
          H_AUTH = {"Authorization": f"Bearer {TOKEN}"}
          H_JSON = {"Authorization": f"Bearer {TOKEN}", "Content-Type": "application/json"}
          enc_table = urllib.parse.quote(TABLE, safe="")
          url = f"https://api.airtable.com/v0/{BASE}/{enc_table}?typecast=true"

          # discover current fields
          def discover_fields():
              r = requests.get(f"https://api.airtable.com/v0/meta/bases/{BASE}/tables", headers=H_AUTH, timeout=30)
              if r.status_code == 200:
                  for t in r.json().get("tables", []):
                      if t.get("id") == TABLE or t.get("name") == TABLE:
                          return [f["name"] for f in t.get("fields", [])]
              r = requests.get(f"https://api.airtable.com/v0/{BASE}/{enc_table}?maxRecords=5", headers=H_AUTH, timeout=30)
              if r.status_code == 200:
                  seen=set()
                  for rec in r.json().get("records", []):
                      seen.update((rec.get("fields") or {}).keys())
                  return sorted(seen) if seen else []
              return []

          avail = discover_fields()
          print("[UPLOAD] Available fields:", avail or "(none detected)")

          # helpers
          def pick_one(cands):
              for c in cands:
                  for f in avail:
                      if f.lower() == c.lower(): return f
              for c in cands:
                  cl=c.lower()
                  for f in avail:
                      if cl in f.lower(): return f
              return None

          def pick_all(cands):
              out, seen=[], set()
              for c in cands:
                  for f in avail:
                      if f.lower()==c.lower() and f not in seen:
                          out.append(f); seen.add(f)
              for c in cands:
                  cl=c.lower()
                  for f in avail:
                      if cl in f.lower() and f not in seen:
                          out.append(f); seen.add(f)
              return out

          # choose your actual fields dynamically (covers snake_case + variations)
          full_name_field  = pick_one(["full_name","Full Name","Name","fullname","Full name","title"])
          cluster_field    = pick_one(["professional_cluster","profession_cluster","professional cluster","Profession Cluster","cluster","Cluster","category","Category","group","Group"])
          occupation_field = pick_one(["occupation","professional_canonical","professional_canonoical","profession","job","role","Role"])
          language_fields  = pick_all(["language","Language","language-origin","Language Origin","lang","Lang"])

          print("[UPLOAD] Mapping chosen:", {
              "full_name": full_name_field,
              "cluster":   cluster_field,
              "occupation":occupation_field,
              "language_targets": language_fields
          })

          # build records from CSV → only to fields that actually exist
          recs=[]
          with open("data/candidates_raw.csv", newline="", encoding="utf-8") as f:
              reader=csv.DictReader(f)
              for row in reader:
                  fields={}
                  if full_name_field  and row.get("full_name"):  fields[full_name_field]  = row["full_name"]
                  if cluster_field    and row.get("cluster"):    fields[cluster_field]    = row["cluster"]
                  if occupation_field and row.get("occupation"): fields[occupation_field] = row["occupation"]
                  if language_fields  and row.get("lang"):
                      for lf in language_fields: fields[lf] = row["lang"]
                  # as a last resort, set a likely primary name so record isn’t empty
                  if not fields:
                      nf = pick_one(["Name","Full Name","full_name","Title","title"])
                      if nf: fields[nf] = row.get("full_name") or "Record"
                  recs.append({"fields": fields})

          total=0
          for i in range(0, len(recs), 10):
              batch = recs[i:i+10]
              r = requests.post(url, headers=H_JSON, data=json.dumps({"records": batch}), timeout=60)
              if r.status_code >= 400:
                  print(r.text); sys.exit(1)
              total += len(batch)
              print(f"[UPLOAD] batch {i//10+1}: {len(batch)}")
              time.sleep(0.25)
          print(f"[UPLOAD] inserted {total} rows")
          PY
